{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83705ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a704f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"EH1OGFJKB5ESXU34\"\n",
    "POLYGON_API_KEY = \"4247kuYEqP7gcbp5dfn7WNIn8DJCEcN_\"\n",
    "DATA_PATH = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07fb07",
   "metadata": {},
   "source": [
    "# Fetching all Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "Listings_URL = f\"https://www.alphavantage.co/query?function=LISTING_STATUS&apikey={API_KEY}\"\n",
    "with requests.Session() as s:\n",
    "    download = s.get(Listings_URL)\n",
    "    decoded_content = download.content.decode('utf-8')\n",
    "    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "    with open(f'{DATA_PATH}/tickers/active_tickers.csv', 'w+', newline='') as f:\n",
    "        basic_writer = csv.writer(f)\n",
    "        basic_writer.writerows(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(f'{DATA_PATH}/tickers/active_tickers.csv')\n",
    "raw_df = raw_df.drop([\"delistingDate\", \"status\"], axis = 1)\n",
    "raw_df = raw_df[raw_df['assetType'] == 'Stock']\n",
    "raw_df = raw_df[(raw_df['exchange'] == 'NYSE') | (raw_df['exchange'] == 'NASDAQ')]\n",
    "raw_df = raw_df[raw_df['symbol'].str.contains('-') == False]\n",
    "raw_df = raw_df[raw_df['symbol'].str.len() <= 4]\n",
    "print(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70c885",
   "metadata": {},
   "source": [
    "# Fetching Market Capitalization for all stock tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb6192",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{DATA_PATH}/tickers/stock_companies.csv', 'w+', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    header = ['Symbol', 'Name', 'CIK', 'Sector', 'Industry', 'MarketCapitalization', 'EBITDA', 'PERatio']\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    unprocessed = []\n",
    "    i = 0\n",
    "    for row in raw_df.iterrows():\n",
    "        \n",
    "        print(i)\n",
    "        i = i+1\n",
    "        \n",
    "        symbol = row[1]['symbol']\n",
    "        url = f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={API_KEY}'\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        \n",
    "        if r.status_code != 200:\n",
    "            unprocessed.append(symbol)\n",
    "            print(f\"Incorrect status code for symbol {symbol}. Expected 200, received {r.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = r.json()\n",
    "            \n",
    "        if not 'Symbol' in data:\n",
    "            unprocessed.append(symbol)\n",
    "            print(f\"Problem with symbol {symbol}. Received \\n{data}\")\n",
    "            continue\n",
    "        \n",
    "        info = [data['Symbol'], data['Name'], data['CIK'], data['Sector'], data['Industry'], data['MarketCapitalization'], data['EBITDA'], data['PERatio']]\n",
    "        writer.writerow(info) \n",
    "        time.sleep(1)\n",
    "        \n",
    "    print(\"all data fetched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9ed20",
   "metadata": {},
   "source": [
    "# Filter and Sort the stock tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7426a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.read_csv(f'{DATA_PATH}/tickers/stock_companies.csv')\n",
    "stock_df = stock_df[stock_df['MarketCapitalization'] != \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['MarketCapitalization'] = pd.to_numeric(stock_df['MarketCapitalization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df[stock_df['MarketCapitalization'] > 300 * 1000 * 1000]\n",
    "print(f'There are {len(stock_df)} stocks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6175a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 unique sectors\n",
      "There are 396 unique industries\n"
     ]
    }
   ],
   "source": [
    "sectors = stock_df['Sector'].unique()\n",
    "print(f'There are {len(sectors)} unique sectors')\n",
    "industries = stock_df['Industry'].unique()\n",
    "print(f'There are {len(industries)} unique industries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f61954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LIFE SCIENCES' 'MANUFACTURING' 'REAL ESTATE & CONSTRUCTION'\n",
      " 'TRADE & SERVICES' 'ENERGY & TRANSPORTATION' 'FINANCE' 'TECHNOLOGY']\n"
     ]
    }
   ],
   "source": [
    "print(sectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.to_csv(f'{DATA_PATH}/tickers/filtered_stock_companies.csv', index = False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac71cb",
   "metadata": {},
   "source": [
    "# Russell 3000 Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3093d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(f'{DATA_PATH}/tickers/stock_companies.csv')\n",
    "ru3000_df = pd.read_csv(f'{DATA_PATH}/tickers/ru3000.csv')\n",
    "ru3000_df = ru3000_df.drop(['Company'], axis = 1)\n",
    "joined_df = pd.merge(all_df, ru3000_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ed750",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tickers = pd.read_csv(f'{DATA_PATH}/tickers/filtered_stock_companies.csv')\n",
    "filtered_tickers = filtered_tickers.drop(['Name','CIK','Sector','Industry','MarketCapitalization','EBITDA','PERatio'],axis=1)\n",
    "joined_df = pd.merge(filtered_tickers, joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37657025",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "joined_df = joined_df.drop_duplicates(subset=['Symbol'])\n",
    "# joined_df = joined_df.drop(columns=['PERatio'])\n",
    "joined_df.to_csv(f'{DATA_PATH}/tickers/filtered_ru3000.csv', index = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ab38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c38d95",
   "metadata": {},
   "source": [
    "# Fetching Daily Adjusted Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd9926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = f'{DATA_PATH}/daily'\n",
    "\n",
    "def fetch_daily_data_to_csv(symbol):\n",
    "    path = f'{folder_path}/{symbol}.csv'\n",
    "    \n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={symbol}&datatype=csv&outputsize=full&apikey={API_KEY}'\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Incorrect status code for symbol {symbol}. Expected 200, received {r.status_code}\")\n",
    "        return False\n",
    "\n",
    "    data = r.text\n",
    "\n",
    "    if not 'timestamp' in data:\n",
    "        print(f\"Problem with symbol {symbol}. Received \\n{data}\")\n",
    "        return False\n",
    "\n",
    "    cr = csv.reader(data.splitlines(), delimiter=',')\n",
    "\n",
    "    with open(path, 'w+', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(cr)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "filtered_df = pd.read_csv(f'{DATA_PATH}/tickers/filtered_ru3000.csv')\n",
    "filtered_df = filtered_df.sort_values(by=['MarketCapitalization'], ascending=False)\n",
    "\n",
    "i = 0\n",
    "for (id, row) in filtered_df.iterrows():\n",
    "    symbol = row[0]\n",
    "    path = f'{folder_path}/{symbol}.csv'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        i = i+1\n",
    "        print(f'{i}/{len(filtered_df)} symbol {symbol} already exists')\n",
    "        continue\n",
    "    \n",
    "    successful = fetch_daily_data_to_csv(symbol);\n",
    "    if (successful):\n",
    "        i = i+1\n",
    "        print(f'{i}/{len(filtered_df)} symbol {symbol} fetch complete')\n",
    "    else:\n",
    "        time.sleep(6)\n",
    "        successful = fetch_daily_data_to_csv(symbol);\n",
    "        if (successful):\n",
    "            i = i+1\n",
    "            print(f'{i}/{len(filtered_df)} symbol {symbol} fetch complete')\n",
    "\n",
    "print(f\"{i}/{len(filtered_df)} data fetched\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86575e1",
   "metadata": {},
   "source": [
    "# Fetching Earnings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aae49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f'{DATA_PATH}/earnings_report'\n",
    "\n",
    "def fetch_earnings_data_to_csv(symbol):\n",
    "    path = f'{folder_path}/{symbol}.csv'\n",
    "    \n",
    "    url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={symbol}&apikey={API_KEY}'\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Incorrect status code for symbol {symbol}. Expected 200, received {r.status_code}\")\n",
    "        return False\n",
    "\n",
    "    data = r.json()\n",
    "\n",
    "    if not 'quarterlyEarnings' in data:\n",
    "        print(f\"Problem with symbol {symbol}. Received \\n{data}\")\n",
    "        return False\n",
    "\n",
    "    data = data['quarterlyEarnings']\n",
    "\n",
    "    field_names = ['fiscalDateEnding', 'reportedDate', 'reportedEPS', 'estimatedEPS', 'surprise', 'surprisePercentage']\n",
    "\n",
    "    with open(path, 'w') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=field_names)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "filtered_df = pd.read_csv(f'{DATA_PATH}/tickers/filtered_ru3000.csv')\n",
    "\n",
    "i = 0\n",
    "for (id, row) in filtered_df.iterrows():\n",
    "    symbol = row[0]\n",
    "    path = f'{folder_path}/{symbol}.csv'\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        i = i+1\n",
    "        print(f'{i}/{len(filtered_df)} symbol {symbol} already exists')\n",
    "        continue\n",
    "\n",
    "    successful = fetch_earnings_data_to_csv(symbol)\n",
    "    if (successful):\n",
    "        i = i+1\n",
    "        print(f'{i}/{len(filtered_df)} symbol {symbol} fetch complete')\n",
    "    else:\n",
    "        time.sleep(6)\n",
    "        successful = fetch_earnings_data_to_csv(symbol)\n",
    "        if (successful):\n",
    "            i = i+1\n",
    "            print(f'{i}/{len(filtered_df)} symbol {symbol} fetch complete')\n",
    "\n",
    "print(f\"{i}/{len(filtered_df)} data fetched\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447ab00",
   "metadata": {},
   "source": [
    "# Fetching Intraday Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = '5min'\n",
    "folder_path = f'{DATA_PATH}/intraday_{interval}'\n",
    "\n",
    "filtered_df = pd.read_csv(f'{DATA_PATH}/tickers/filtered_ru3000.csv')\n",
    "filtered_df = filtered_df.sort_values(by=['MarketCapitalization'], ascending=False)\n",
    "\n",
    "def try_fetch(url, symbol):\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        print(f\"Incorrect status code for symbol {symbol}. Expected 200, received {r.status_code}\")\n",
    "        return (False, False)\n",
    "\n",
    "    def process_request(r):\n",
    "        try:\n",
    "            decoded_content = r.content.decode('utf-8')\n",
    "            raw_data = StringIO(decoded_content)\n",
    "            return pd.read_csv(raw_data)\n",
    "        except:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    df = process_request(r)\n",
    "    if not 'time' in df:\n",
    "        print(\"sleep for 6 seconds due to API limit\")\n",
    "        time.sleep(6)\n",
    "        \n",
    "        r = requests.get(url)\n",
    "        df = process_request(r)\n",
    "        if not 'time' in df:\n",
    "            print(f\"Problem with symbol {symbol}. Received \\n{data}\")\n",
    "            return (False, False)\n",
    "    return (True, r)\n",
    "\n",
    "i = 0\n",
    "incomplete_stocks = []\n",
    "for (id, row) in filtered_df.iterrows():\n",
    "    symbol = row[0]\n",
    "    path = f'{folder_path}/{symbol}.csv'\n",
    "\n",
    "    frames = []\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        i = i+1\n",
    "        print(f'{i}/{len(filtered_df)} symbol {symbol} already exists')\n",
    "        continue\n",
    "        \n",
    "    incomplete_data = False\n",
    "\n",
    "    with requests.Session() as s:\n",
    "        for y in range(1,3):\n",
    "            if (incomplete_data):\n",
    "                break\n",
    "            for m in range(1,13):\n",
    "                url = f'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol={symbol}&interval={interval}&slice=year{y}month{m}&apikey={API_KEY}'\n",
    "                (success, download) = try_fetch(url, symbol)\n",
    "                if success:\n",
    "                    decoded_content = download.content.decode('utf-8')\n",
    "                    raw_data = StringIO(decoded_content)\n",
    "                    df = pd.read_csv(raw_data)\n",
    "                    if (len(df) < 200):\n",
    "                        incomplete_data = True\n",
    "\n",
    "                    frames.append(df)\n",
    "                else:\n",
    "                    incomplete_data = True\n",
    "\n",
    "                if (incomplete_data):\n",
    "                    break\n",
    "            if (incomplete_data):\n",
    "                break\n",
    "\n",
    "        if (incomplete_data):\n",
    "            incomplete_stocks.append(symbol)\n",
    "            print(f'{symbol} is problematic')\n",
    "            continue\n",
    "\n",
    "    # dataframe that combines 12 months worth of data\n",
    "    df = pd.concat(frames)\n",
    "    # round off the decimal places\n",
    "    df['open'] = df['open'].astype(float).round(3)\n",
    "    df['high'] = df['high'].astype(float).round(3)\n",
    "    df['low'] = df['low'].astype(float).round(3)\n",
    "    df['close'] = df['close'].astype(float).round(3)\n",
    "    \n",
    "    df.to_csv(path, sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "    i = i+1\n",
    "    print(f'{i}/{len(filtered_df)} symbol {symbol} fetch complete')\n",
    "    \n",
    "print(f\"{i}/{len(filtered_df)} data fetched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9126081",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f'{DATA_PATH}/daily'\n",
    "\n",
    "def fetch_daily_data_to_csv(symbol):\n",
    "    path = f'{folder_path}/{symbol}.csv'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        print(f'symbol {symbol} already exists')\n",
    "        return\n",
    "        \n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={symbol}&datatype=csv&outputsize=full&apikey={API_KEY}'\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Incorrect status code for symbol {symbol}. Expected 200, received {r.status_code}\")\n",
    "        return\n",
    "\n",
    "    data = r.text\n",
    "\n",
    "    if not 'timestamp' in data:\n",
    "        print(f\"Problem with symbol {symbol}. Received \\n{data}\")\n",
    "        return\n",
    "\n",
    "    cr = csv.reader(data.splitlines(), delimiter=',')\n",
    "\n",
    "    with open(path, 'w+', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(cr)\n",
    "\n",
    "    return True\n",
    "\n",
    "fetch_daily_data_to_csv('QQQ')\n",
    "fetch_daily_data_to_csv('SPY')\n",
    "fetch_daily_data_to_csv('IWM')\n",
    "fetch_daily_data_to_csv('TLT')\n",
    "fetch_daily_data_to_csv('GOVT')\n",
    "fetch_daily_data_to_csv('VTI')\n",
    "fetch_daily_data_to_csv('VXX')\n",
    "fetch_daily_data_to_csv('VXZ')\n",
    "fetch_daily_data_to_csv('VIXY')\n",
    "fetch_daily_data_to_csv('VIXM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22597d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
